% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Transformer.R
\name{setTransformer}
\alias{setTransformer}
\title{create settings for training a non-temporal transformer}
\usage{
setTransformer(
  numBlocks = 3,
  dimToken = 96,
  dimOut = 1,
  numHeads = 8,
  attDropout = 0.25,
  ffnDropout = 0.25,
  resDropout = 0,
  dimHidden = 512,
  weightDecay = 1e-06,
  learningRate = 3e-04,
  batchSize = 1024,
  epochs = 10,
  device = "cpu",
  hyperParamSearch = "random",
  randomSamples = 100,
  seed = NULL
)
}
\arguments{
\item{numBlocks}{number of transformer blocks}

\item{dimToken}{dimension of each token (embedding size)}

\item{dimOut}{dimension of output, usually 1 for binary problems}

\item{numHeads}{number of attention heads}

\item{attDropout}{dropout to use on attentions}

\item{ffnDropout}{dropout to use in feedforward block}

\item{resDropout}{dropout to use in residual connections}

\item{dimHidden}{dimension of the feedworward block}

\item{weightDecay}{weightdecay to use}

\item{learningRate}{learning rate to use}

\item{batchSize}{batchSize to use}

\item{epochs}{How many epochs to run the model for}

\item{device}{Which device to use, cpu or cuda}

\item{hyperParamSearch}{what kind of hyperparameter search to do, default 'random'}

\item{randomSamples}{How many samples to use in hyperparameter search if random}

\item{seed}{Random seed to use}
}
\description{
A transformer model
}
\details{
from https://arxiv.org/abs/2106.11959
}
