% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Transformer.R
\name{setTransformer}
\alias{setTransformer}
\title{create settings for training a transformer}
\usage{
setTransformer(
  numBlocks = 3,
  dimToken = 192,
  dimOut = 1,
  numHeads = 8,
  attDropout = 0.2,
  ffnDropout = 0.1,
  dimHidden = 256,
  dimHiddenRatio = NULL,
  attnImplementation = "sdpa",
  temporal = FALSE,
  temporalSettings = list(positionalEncoding = list(name = "SinusoidalPE", dropout =
    0.1), maxSequenceLength = 256, truncation = "tail", timeTokens = TRUE),
  estimatorSettings = setEstimator(weightDecay = 1e-06, batchSize = 1024, epochs = 10,
    seed = NULL),
  hyperParamSearch = "random",
  randomSample = 1,
  randomSampleSeed = NULL
)
}
\arguments{
\item{numBlocks}{number of transformer blocks}

\item{dimToken}{dimension of each token (embedding size)}

\item{dimOut}{dimension of output, usually 1 for binary
problems}

\item{numHeads}{number of attention heads}

\item{attDropout}{dropout to use on attentions}

\item{ffnDropout}{dropout to use in feedforward block}

\item{dimHidden}{dimension of the feedworward block}

\item{dimHiddenRatio}{dimension of the feedforward block as a ratio
of dimToken (embedding size)}

\item{attnImplementation}{attention implementation to use, either
'sdpa' (scaled dot product attention from pytorch) or 'flash" (flash attention v2
from `flash-attn` package). Flash requires a new GPU with compute capability 8.0
or higher, bfloat16 and the `flash-attn` package to be installed.
If you chose flash attention you need to force the which python environment 
is and need to make sure it has all required packages installed.}

\item{temporal}{Whether to use a transformer with temporal data}

\item{temporalSettings}{settings for the temporal transformer. Which include
- `positionalEncoding`: Positional encoding to use, either a character
  or a list with name and settings, default 'SinusoidalPE' with dropout 0.1
- `maxSequenceLength`: Maximum sequence length, sequences longer than This
  will be truncated and/or padded to this length either a number or 'max' for the Maximum
- `truncation`: Truncation method, only 'tail' is supported
- `timeTokens`: Whether to use time tokens, default TRUE}

\item{estimatorSettings}{created with `setEstimator`}

\item{hyperParamSearch}{what kind of hyperparameter search to do,
default 'random'}

\item{randomSample}{How many samples to use in hyperparameter
search if random}

\item{randomSampleSeed}{Random seed to sample hyperparameter
combinations}
}
\value{
list of settings for the transformer model
}
\description{
A transformer model
}
\details{
The non-temporal transformer is from https://arxiv.org/abs/2106.11959
}
